{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac26811",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7013165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe348b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0aa39",
   "metadata": {},
   "source": [
    "# Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/Users/mnoor/Desktop/FaceMaskProject/FaceMask/img'\n",
    "CATEGORIES = ['with_mask','without_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "IMG_SIZE=128\n",
    "def making_trian_dataset():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(data_path, category) # path to our data\n",
    "        class_num = CATEGORIES.index(category)# classifcation index\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # resizing our imgz\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "making_trian_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)\n",
    "train_data, labels_data = zip(*training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Sample in training_data[:10]:\n",
    "    print(Sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf237df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X)#.reshape(-1, IMG_SIZE,IMG_SIZE,3)\n",
    "y = to_categorical(y, num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d514da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038b423",
   "metadata": {},
   "source": [
    "Data Splitting For Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample,nx,ny,ni=X.shape\n",
    "X1=X.reshape(nsample,nx*ny*ni)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_val_test, y1_train, y1_val_test = train_test_split(X1, y, test_size=.2, random_state=77)\n",
    "X1_val, X1_test, y1_val, y1_test = train_test_split(X1_val_test, y1_val_test, test_size=0.5, random_state=77)\n",
    "print(f\"\\nTraining data: {X1_train.shape},  labels: {y1_train.shape}\")\n",
    "print(f\"Validation data: {X1_val.shape},  labels: {y1_val.shape}\")\n",
    "print(f\"Testing data: {X1_test.shape},  labels: {y1_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45701a",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=.2, random_state=77)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=77)\n",
    "print(f\"\\nTraining data: {X_train.shape},  labels: {y_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape},  labels: {y_val.shape}\")\n",
    "print(f\"Testing data: {X_test.shape},  labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2208e00",
   "metadata": {},
   "source": [
    "Visualizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask_path='/Users/mnoor/Desktop/FaceMaskProject/FaceMask/img/with_mask'\n",
    "No_mask='/Users/mnoor/Desktop/FaceMaskProject/FaceMask/img/without_mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[go.Pie(labels=['with_mask','without_mask'], \n",
    "        values=[len(os.listdir(Mask_path)),len(os.listdir(No_mask))])\n",
    "    ])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeff2e1",
   "metadata": {},
   "source": [
    "Distribution of the target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_set(data, labels, classes):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(10):\n",
    "        plt.subplot(5, 5, (i+1))\n",
    "        random_val = np.random.randint(low=0, high=len(data))\n",
    "        img = data[random_val]\n",
    "        plt.imshow(img)\n",
    "        plt.axis(False)\n",
    "        plt.title(classes[np.argmax(labels[random_val])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_set(data=X_train, labels=y_train, classes=CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf19eb2",
   "metadata": {},
   "source": [
    "Model Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96003ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from keras.applications.vgg19 import VGG19  \n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Dropout, Flatten,InputLayer\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications import mobilenet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de9dc5",
   "metadata": {},
   "source": [
    "MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ff497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False, input_shape=(128,128,3)) \n",
    " \n",
    "for layer in model_3.layers:\n",
    "    layer.trainable = False    \n",
    "\n",
    "x = model_3.output\n",
    "x = Flatten()(x) \n",
    "x = Dense(100, activation='relu')(x) \n",
    "x = Dense(50, activation='relu')(x)\n",
    "predictions = Dense(3, activation='softmax')(x) \n",
    "\n",
    "model_3 = Model(inputs=model_3.input, outputs=predictions)\n",
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.fit(X_train, y_train, validation_data=(X_val,y_val),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a7623",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68eb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = VGG19(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  \n",
    "\n",
    "# Freeze convolutional layers\n",
    "for layer in model_6.layers:\n",
    "    layer.trainable = False  \n",
    "\n",
    "NN_transfer_19 = Sequential(\n",
    "                        [InputLayer(input_shape=(128,128,3)),model_6,\n",
    "                         Flatten(),  # should be fine , or add layers\n",
    "                         Dense(128, activation='relu'),\n",
    "                         Dense(64, activation='relu'),\n",
    "                         Dense(32, activation='relu'),   # 2 dense is must bcuz VGG16 model Conv2D twice and Maxpooling -> get a lot more features\n",
    "                         Dense(3, activation='softmax')]\n",
    "                       )\n",
    "\n",
    "NN_transfer_19.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'],)\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_V19 = NN_transfer_19.fit(X_train,y_train, validation_data=(X_val,y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = result_V19.history['accuracy']\n",
    "val_acc = result_V19.history['val_accuracy']\n",
    "\n",
    "loss = result_V19.history['loss']\n",
    "val_loss = result_V19.history['val_loss']\n",
    "\n",
    "epochs=10\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy', color = 'red') #, color = 'red')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy',color='blue')  # , color='blue')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss',color='red')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss',color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4833bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] saving mask detector model...\")\n",
    "model_6.save(\"Face-Mask-Detection-model_6.model\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a75ec",
   "metadata": {},
   "source": [
    "# Video detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55aa6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the config file \n",
    "config_file = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'\n",
    "\n",
    "# This is from the weight file \n",
    "frozen_model = 'frozen_inference_graph.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cefd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance from model detection\n",
    "\n",
    "model = cv2.dnn_DetectionModel(frozen_model, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "file_name = 'labels.txt'\n",
    "with open(file_name, 'rt') as fpt:\n",
    "    class_labels = fpt.read().rstrip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setInputSize(320,320)\n",
    "# Note: 255/2 = 127.5\n",
    "model.setInputScale(1.0/127.5)\n",
    "\n",
    "model.setInputMean((127.5,127.5,127.5))\n",
    "\n",
    "model.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record any vidoe and add it here for a demo to the students \n",
    "import cv2\n",
    "#cap = cv2.VideoCapture('videos/car_traffic.mp4')\n",
    "cap = cv2.VideoCapture('/Users/mnoor/Desktop/FaceMaskProject/Video/Video1.mp4')\n",
    "\n",
    "# Check if video is open correclty \n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "if not cap.isOpened():\n",
    "    raise IOError('Cannot Open Video File')\n",
    "    \n",
    "# Text font\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# Font scale\n",
    "fontScale = 2 \n",
    "# Box color\n",
    "box_color = (255, 0, 0)\n",
    "# Teaxt color\n",
    "text_color = (0, 255, 0)\n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "   \n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    index, confidence, bbox = model.detect(frame)\n",
    "    \n",
    "    print(index)\n",
    "    if(len(index) !=0):\n",
    "        for index, conf, box in zip(index.flatten(), confidence.flatten(), bbox):\n",
    "            if(index <=2):\n",
    "                cv2.rectangle(frame, box, (255,0,0), 2)\n",
    "                cv2.putText(frame, class_labels[index-1],(box[0]+10, box[1]+40), font, fontScale = fontScale, color = text_color, thickness = thickness)\n",
    "\n",
    "    cv2.imshow('Object Detection Demo', frame)\n",
    "    \n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f818aa2",
   "metadata": {},
   "source": [
    "# LIVE Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1a40ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff0e69b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39mSequential([\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      3\u001b[0m     MaxPooling2D(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      4\u001b[0m     \n\u001b[1;32m      5\u001b[0m     Conv2D(\u001b[38;5;241m100\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m     MaxPooling2D(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     Flatten(),\n\u001b[1;32m      9\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     10\u001b[0m     Dense(\u001b[38;5;241m50\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     11\u001b[0m     Dense(\u001b[38;5;241m2\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/legacy/interfaces.py:91\u001b[0m, in \u001b[0;36mgenerate_legacy_interface.<locals>.legacy_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     signature \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     89\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate your `\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m object_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` call to the \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     90\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeras 2 API: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m signature, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional.py:473\u001b[0m, in \u001b[0;36mConv2D.__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;129m@interfaces\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_conv2d_support\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filters,\n\u001b[1;32m    458\u001b[0m              kernel_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m              bias_constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    472\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivity_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/layers/convolutional.py:105\u001b[0m, in \u001b[0;36m_Conv.__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, rank,\n\u001b[1;32m     89\u001b[0m              filters,\n\u001b[1;32m     90\u001b[0m              kernel_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m              bias_constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_Conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m=\u001b[39m rank\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m=\u001b[39m filters\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py:132\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[1;32m    131\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 132\u001b[0m     name \u001b[38;5;241m=\u001b[39m _to_snake_case(prefix) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_uid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/backend/tensorflow_backend.py:74\u001b[0m, in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"Get the uid for the default graph.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m# Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    A unique identifier for the graph.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _GRAPH_UID_DICTS\n\u001b[0;32m---> 74\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _GRAPH_UID_DICTS:\n\u001b[1;32m     76\u001b[0m     _GRAPH_UID_DICTS[graph] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "model =Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = \"/Users/mnoor/Desktop/FaceMaskProject/FaceMask/train\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72280b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR = \"/Users/mnoor/Desktop/FaceMaskProject/FaceMask/test\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0170284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2022-11-05 20:56:42.550670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Unable to open file (file read failed: time = Sat Nov  5 20:56:54 2022\n, filename = 'model2-010.model', file descriptor = 77, errno = 21, error message = 'Is a directory', buf = 0x7ffee36453f8, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel2-010.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m labels_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithout mask\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      7\u001b[0m color_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m),\u001b[38;5;241m1\u001b[39m:(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m)}\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/saving.py:417\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    415\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    416\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mGroup)\n\u001b[0;32m--> 417\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mh5dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     model \u001b[38;5;241m=\u001b[39m _deserialize_model(f, custom_objects, \u001b[38;5;28mcompile\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/io_utils.py:186\u001b[0m, in \u001b[0;36mH5Dict.__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:507\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds)\u001b[0m\n\u001b[1;32m    502\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    503\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    504\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    505\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    506\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 507\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:220\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    219\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 220\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    222\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Unable to open file (file read failed: time = Sat Nov  5 20:56:54 2022\n, filename = 'model2-010.model', file descriptor = 77, errno = 21, error message = 'Is a directory', buf = 0x7ffee36453f8, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "#model=load_model(\"model2-010.model\")\n",
    "\n",
    "labels_dict={0:'without mask',1:'mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736dfc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1)\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "    for f in faces: \n",
    "        (x, y, w, h) = [v * size for v in f]\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1) \n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "    cv2.imshow('LIVE',im)\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27: #The Esc key \n",
    "        break\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94919696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
